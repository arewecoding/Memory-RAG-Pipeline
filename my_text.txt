## Linear Transformations, Null Spaces, and Ranges
### Linear Transformation
Let $V$ and $W$ be vector spaces over the same field $F$. We call a function $T:V\rightarrow W$ a **linear transformation from $V$ to $W$** if, for all $x, y\in V$ and $c\in F$, we have
1. $T(x+y)=T(x)+T(y)$ and
2. $T(cx)=cT(x)$.
Here, we say $T$ is **linear**. ^linear
##### Properties of A Linear Transformation
1. If $T$ is linear, then $T(0)=0$.
2. $T$ is linear if and only if $T(cx+y)=cT(x)+T(y)$ for all $x,y\in V$ and $c\in F$.
3. If $T$ is linear, then $T(x-y)=T(x)-T(y)$ for all $x,y\in V$
4. $T$ is linear if and only if, for $x_{1}, x_{2}, ... , a_{n}\in F$, we have$$T(\sum_{i=1}^{n} a_{i}x_{i})= \sum_{i=1}^{n} a_{i}T(x_{i})$$
##### Identity Transformation
For vector spaces $V$ and $W$ (over $F$) the identity transformation $I_{v}:V\rightarrow V$ by $I_{v}(x)=x$ for all $x\in V$
##### Zero Transformation
For vector spaces $V$ and $W$ (over $F$) the zero transformation $T_{0}:V\rightarrow V$ by $T_{0}(x)=0$ for all $x\in V$
### Null Space
Let $V$ and $W$ be vector spaces, and let $T:V\rightarrow W$ be linear. We define the **null space** (or **kernel**) $N(T)$ of $T$ to be the set of all vectors $x$ in $V$ such that $T(x)=0$; that is, $N(T)=\{ x\in V:T(x)=0 \}$. ^kernel
### Range
We define the **range** (or **image**) $R(T)$ of $T$ to be the subset of $W$ consisting of all images (under $T$) of vectors in $V$; that is, $R(T)=\{ T(x):x\in V \}$. ^image
### $Theorem(2.1)$
Let $V$ and $W$ be vector spaces and $T:V\rightarrow W$ be [linear](Linear%20Transformations%20and%20Matrices#^linear). Then $N(T)$ and $R(T)$ are subspaces of $V$ and $W$, respectively.

>[!tip]- Proof
>To clarify the notation, we use the symbols $0_V$ and $0_W$ to denote the zero vectors of $V$ and $W$, respectively.
>
Since $T(0_V) = 0_W$, we have that $0_V \in N(T)$. Let $x,y \in N(T)$ and $c \in F$. Then $T(x+y) = T(x) + T(y) = 0_W + 0_W = 0_W$, and $T(cx) = cT(x) = c0_W = 0_W$. Hence $x + y \in N(T)$ and $cx \in N(T)$, so that $N(T)$ is a subspace of $V$.
>
Because $T(0_V) = 0_W$, we have that $0_W \in R(T)$. Now let $x, y \in R(T)$ and $c \in F$. Then there exist $v$ and $w$ in $V$ such that $T(v) = x$ and $T(w) = y$. So $T(v+w) = T(v) + T(w) = x + y$, and $T(cv) = cT(v) = cx$. Thus $x + y \in R(T)$ and $cx \in R(T)$, so $R(T)$ is a subspace of $W$.
### $Theorem(2.2)$
Let $V$ and $W$ be vector spaces, and let $T:V\rightarrow W$ be [linear](Linear%20Transformations%20and%20Matrices#^linear). If $\beta = \{ v_1, v_2, ... , v_n \}$ is a basis for $V$, then$$R(T)=\textrm{span} (T(\beta))=\textrm{span} (\{ T(v_1), T(v_2), ... , T(v_n) \}).$$
>[!tip]- Proof
>Clearly $T(v_i) \in R(T)$ for each $i$. Because $R(T)$ is a subspace, $R(T)$ contains $\text{span}(\{T(v_1), T(v_2), \ldots, T(v_n)\}) = \text{span}(T(\beta))$ by Theorem 1.5 (p. 31).
>
>Now suppose that $w \in R(T)$. Then $w = T(v)$ for some $v \in V$. Because $\beta$ is a basis for $V$, we have
>
>$$v = \sum_{i=1}^{n} a_i v_i \quad \text{for some } a_1, a_2, \ldots, a_n \in F.$$
>
>Since $T$ is linear, it follows that
>
>$$w = T(v) = \sum_{i=1}^{n} a_i T(v_i) \in \text{span}(T(\beta)).$$
>
>So $R(T)$ is contained in $\text{span}(T(\beta))$
### Definitions:
Let $V$ and $W$ be vector spaces, and let $T:V\rightarrow W$ be linear. If $N(T)$ and $R(T)$ are finite-dimensional, then
#### Nullity
We define **nullity** of $T$, denoted by $\text{nullity} (T)$, to be the dimensions of $N(T)$.
#### Rank
We define **rank** of $T$, denoted by $\text{rank} (T)$, to be the dimensions of $R(T)$.
### $Theorem(2.3)$: Dimension Theorem / Rank-Nullity Theorem
Let $V$ and $W$ be vector spaces, and let $T:V\rightarrow W$ be linear. If $V$ is finite-dimensional, then$$\text{nullity} (T)+\text{rank} (T)=\text{dim} (V)$$
>[!tip]- Proof
>Suppose that $\text{dim}(V) = n$, $\text{dim}(N(T)) = k$, and $\{v_1, v_2, ..., v_k\}$ is a basis for $N(T)$. By the corollary to Theorem 1.11 (p. 51), we may extend $\{v_1, v_2, ..., v_k\}$ to a basis $\beta = \{v_1, v_2, ..., v_n\}$ for $V$. We claim that $S = \{T(v_{k+1}), T(v_{k+2}), ..., T(v_n)\}$ is a basis for $R(T)$.
>
> First we prove that $S$ generates $R(T)$. Using Theorem 2.2 and the fact that $T(v_i) = 0$ for $1 \leq i \leq k$, we have
>
> $$
> R(T) = \text{span}\{T(v_1), T(v_2), ..., T(v_n)\} \\
> = \text{span}\{T(v_{k+1}), T(v_{k+2}), ..., T(v_n)\} = \text{span}(S).
> $$
>
> Now we prove that $S$ is linearly independent. Suppose that
>
> $$
> \sum_{i=k+1}^{n} b_i T(v_i) = 0 \quad \text{for } b_{k+1}, b_{k+2}, ..., b_n \in F.
> $$
>
> Using the fact that $T$ is linear, we have
>
> $$
> T\left( \sum_{i=k+1}^{n} b_i v_i \right) = 0.
> $$
>
> So
>
> $$
> \sum_{i=k+1}^{n} b_i v_i \in N(T).
> $$
>
> Hence there exist $c_1, c_2, ..., c_k \in F$ such that
>
> $$
> \sum_{i=k+1}^{n} b_i v_i = \sum_{i=1}^{k} c_i v_i \quad \text{or} \quad \sum_{i=1}^{k} (-c_i)v_i + \sum_{i=k+1}^{n} b_i v_i = 0.
> $$
>
>Since $\beta$ is a basis for $V$, we have $b_{i}=0$ for all $i$. Hence $S$ is linearly independent. Notice that this argument also shows that $T(v_{k+1}), T(v_{k+2}), ... ,T(v_{n})$ are distinct; therefore $\text{rank} (T)=n-k$
>
### $Theorem(2.4)$:
Let $V$ and $W$ be vector spaces, and let $T:V\rightarrow W$ be linear. Then $T$ is one-to-one if and only if $N(T)={0}$.

> [!tip]- Proof
> Suppose that $T$ is one-to-one and $x \in N(T)$. Then $T(x) = 0 = T(0)$. Since $T$ is one-to-one, we have $x = 0$. Hence $N(T) = \{0\}$.
>
> Now assume that $N(T) = \{0\}$, and suppose that $T(x) = T(y)$. Then $0 = T(x) - T(y) = T(x - y)$ by property 3 on page 65. Therefore $x - y \in N(T) = \{0\}$. So $x - y = 0$, or $x = y$. This means that $T$ is one-to-one.
### $Theorem(2.5)$:
Let $V$ and $W$ be finite-dimensional vector spaces of equal dimension, and let $T:V\rightarrow W$ be linear. Then the following are equivalent.
1. $T$ is one-to-one.
2. $T$ is onto.
3. $\text{rank} (T)=\text{dim} (V)$.

> [!tip]- Proof
> From the dimension theorem, we have
>
> $$
> \text{nullity}(T) + \text{rank}(T) = \text{dim}(V).
> $$
>
> Now, with the use of Theorem 2.4, we have that $T$ is one-to-one if and only if $N(T) = \{0\}$, if and only if nullity$(T) = 0$, if and only if rank$(T) = \text{dim}(V)$, if and only if rank$(T) = \text{dim}(W)$, and if and only if dim$(R(T)) = \text{dim}(W)$. By Theorem 1.11 (p. 50), this equality is equivalent to $R(T) = W$, the definition of $T$ being onto.
### $Theorem(2.6)$:
Let $V$ and $W$ be vector spaces over $F$, and suppose that ${v_1, v_2, ... , v_n}$ is a basis for $V$. For ${w_1, w_2, ... , w_n}$ in $W$, there exists exactly one linear transformation $T:V\rightarrow W$ such that $T(v_i)=w_i$ for $i=1,2,...,n$.

> [!tip]- Proof
> Let $x \in V$. Then
> $$
> x = \sum_{i=1}^{n} a_i v_i,
> $$
> where $a_1, a_2, ..., a_n$ are unique scalars. Define
> $$
> T: V \rightarrow W \text{ by } T(x) = \sum_{i=1}^{n} a_i w_i.
> $$
> (a) $T$ is linear: Suppose that $u, v \in V$ and $d \in F$. Then we may write
> $$
> u = \sum_{i=1}^{n} b_i v_i \text{ and } v = \sum_{i=1}^{n} c_i v_i
> $$
> for some scalars $b_1, b_2, ..., b_n, c_1, c_2, ..., c_n$. Thus
> $$
> du + v = \sum_{i=1}^{n} (db_i + c_i)v_i.
> $$
> So
> $$
> T(du + v) = \sum_{i=1}^{n} (db_i + c_i)w_i = d \sum_{i=1}^{n} b_i w_i + \sum_{i=1}^{n} c_i w_i = dT(u) + T(v).
> $$
> (b) Clearly
> $$
> T(v_i) = w_i \text{ for } i = 1, 2, ..., n.
> $$
> (c) $T$ is unique: Suppose that $U: V \rightarrow W$ is linear and $U(v_i) = w_i$ for $i = 1, 2, ..., n$. Then for $x \in V$ with
> $$
> x = \sum_{i=1}^{n} a_i v_i,
> $$
> we have
> $$
> U(x) = \sum_{i=1}^{n} a_i U(v_i) = \sum_{i=1}^{n} a_i w_i = T(x).
> $$
> Hence $U = T$.
#### Corollary
Let $V$ and $W$ be vector spaces, and suppose that $V$ has a finite basis ${v_1, v_2, ... , v_n}$. If $U,T:V\rightarrow W$ are linear and $U(v_i)=T(v_i)$ for $i=1,2,...,n$, then $U=T$.
##### Projection
Let $V$ be a vector space and $W_1$ and $W_2$ be subspaces of $V$ such that $V = W_1 \oplus W_2$. (Recall the definition of direct sum given on page 22.) The function $T: V \rightarrow V$ defined by $T(x) = x_1$ where $x = x_1 + x_2$ with $x_1 \in W_1$ and $x_2 \in W_2$, is called the *projection of V on* $W_1$ or the *projection on* $W_1$ *along* $W_2$.
##### T-invariant
Let $V$ be a vector space, and let $T: V \to V$ be linear. A subspace $W$ of $V$ is said to be **$T$-invariant** if $T(x) \in W$ for every $x \in W$, that is, $T(W) \subseteq W$. If $W$ is $T$-invariant, we define the **restriction of $T$ on $W$** to be the function $T_W: W \to W$ defined by $T_W(x) = T(x)$ for all $x \in W$.
##### Additive Function
A function $T:V\rightarrow W$ between vector spaces $V$ and $W$ is called **additive** if $T(x+y)=T(x)+T(y)$ for all $x,y\in V$.

## The Matric Representation of a Linear Transformation
### Ordered Basis
Let $V$ be a finite-dimensional vector space. An *ordered basis* for $V$ is a basis for $V$ endowed with a specific order; that is, an ordered basis for $V$ is a finite sequence of linearly independent vectors in $V$ that generates $V$.
#### Example 1
In $F^3$, $\beta = \{e_1, e_2, e_3\}$ can be considered an ordered basis. Also $\gamma = \{e_2, e_1, e_3\}$ is an ordered basis, but $\beta \neq \gamma$ as ordered bases.

For the vector space $F^n$, we call $\{e_1, e_2, ..., e_n\}$ the **standard ordered basis** for $F^n$. Similarly, for the vector space $P_n(F)$, we call $\{1, t, ..., t^n\}$ the **standard ordered basis** for $P_n(F)$.
### Coordinate Vector
Let $\beta = \{v_1, v_2, ..., v_n\}$ be an ordered basis for a finite-dimensional vector space $V$. For $x \in V$, let $a_1, a_2, ..., a_n$ be the unique scalars such that
$$
x = \sum_{i=1}^{n} a_i v_i.
$$
We define the *coordinate vector of $x$ relative to $\beta$*, denoted $[x]_\beta$, by
$$
[x]_\beta = \begin{pmatrix}
a_1 \\
a_2 \\
\vdots \\
a_n
\end{pmatrix}.
$$
Notice that $[u_i]_\beta = e_i$ in the preceding definition. It is left as an exercise to show that the correspondence $x \mapsto [x]_\beta$ provides us with a linear transformation from $V$ to $F^n$.
#### Definition
Using the notation above, we call the $m \times n$ matrix $A$ defined by $A_{ij} = a_{ij}$ the **matrix representation of $T$ in the ordered bases $\beta$ and $\gamma$** and write $A = [T]_{\beta}^{\gamma}$. If $V = W$ and $\beta = \gamma$, then we write $A = [T]_{\beta}$. ^matrix-representation-of-tranformation-in-the-ordered-bases

Notice that the $j$th column of $A$ is simply $[T(v_j)]_{\gamma}$. Also observe that if $U: V \rightarrow W$ is a linear transformation such that $[U]_{\gamma} = [T]_{\gamma}$, then $U = T$ by the corollary to Theorem 2.6.
### Identity Matrix
We define the **Kronecker delta** $\delta_{ij}$ by $\delta_{ij} = 1$ if $i = j$ and $\delta_{ij} = 0$ if $i \neq j$. The $n \times n$ **identity matrix** $I_n$ is defined by $(I_n)_{ij} = \delta_{ij}$. When the context is clear, we sometimes omit the subscript $n$ from $I_n$. ^Kronecker-delta

For example,

$$
I_1 = (1), \quad I_2 = \begin{pmatrix}
1 & 0 \\
0 & 1
\end{pmatrix}, \quad \text{and} \quad I_3 = \begin{pmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{pmatrix}.
$$

Thus, the matrix representation of a zero transformation is a zero matrix, and the matrix representation of an identity transformation is an identity matrix.
#### LA Addition & Scalar Multiplication
Let $T, U: V \rightarrow W$ be arbitrary functions, where $V$ and $W$ are vector spaces over $F$, and let $a \in F$. We define $T + U: V \rightarrow W$ by $(T + U)(x) = T(x) + U(x)$ for all $x \in V$, and $aT: V \rightarrow W$ by $(aT)(x) = aT(x)$ for all $x \in V$.
### $Theorem(2.7)$:
Let $V$ and $W$ be vector spaces over a field $F$, and let $T, U: V \rightarrow W$ be linear.
1. For all $a \in F$, $T + U$ is linear.
2. Using the operations of addition and scalar multiplication in the preceding definition, the collection of all linear transformations from $V$ to $W$ is a vector space over $F$.

> [!tip]- Proof
> (1.) Let $x, y \in V$ and $c \in F$. Then $$(aT + U)(cx + y) = aT(cx + y) + U(cx + y)$$ $$= a[T(cx + y)] + cU(x) + U(y)$$ $$= a[cT(x) + T(y)] + cU(x) + U(y)$$ $$= acT(x) + aT(y) + cU(x) + U(y)$$ $$= c[aT(x) + U(x)] + [aT(y) + U(y)]$$ $$= c(aT + U)(x) + (aT + U)(y).$$ So $aT + U$ is linear. (2.) Noting that $T_0$, the zero transformation, plays the role of the zero vector, it is easy to verify that the axioms of a vector space are satisfied, and hence that the collection of all linear transformations from $V$ into $W$ is a vector space over $F$.
#### Vector Space of Transformations
Let $V$ and $W$ be vector spaces over $F$. We denote the vector space of all linear transformations from $V$ into $W$ by $\mathcal{L}(V, W)$. In the case that $V = W$, we write $\mathcal{L}(V)$ instead of $\mathcal{L}(V, V)$.
### $Theorem(2.8)$:
Let $V$ and $W$ be finite-dimensional vector spaces with ordered bases $\beta$ and $\gamma$, respectively, and let $T, U: V \rightarrow W$ be linear transformations. Then:
1. $[T + U]_{\beta} = [T]_{\beta} + [U]_{\beta}$
2. $[aT]_{\beta} = a[T]_{\beta}$ for all scalars $a$.

> [!tip]- Proof
> Let $\beta = \{v_1, v_2, ..., v_n\}$ and $\gamma = \{w_1, w_2, ..., w_m\}$. There exist unique scalars $a_{ij}$ and $b_{ij}$ ($1 \leq i \leq m$, $1 \leq j \leq n$) such that $$T(w_j) = \sum_{i=1}^{m} a_{ij} w_i, \quad \text{and} \quad U(w_j) = \sum_{i=1}^{m} b_{ij} w_i, \text{ for } 1 \leq j \leq n.$$ Hence $$(T + U)(w_j) = \sum_{i=1}^{m} (a_{ij} + b_{ij}) w_i.$$Thus
> $$(T + U)_{\beta}^{j} = a_{ij} + b_{ij} = ([T]_{\beta} + [U]_{\beta})_{ij}.$$
> So (1.) is proved, and the proof of (2.) is similar.
